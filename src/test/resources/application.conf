#Â storage params
s3_ssl_enabled = false
s3_endpoint = "http://127.0.0.1"
s3_path_style_access = true
checkpoint_path = /spark_checkpoint

# read from environment vars
aws_access_key_id = ""
aws_secret_access_key = ""

# locations
bucket_name = "METADATA"
bucket_landing_path = /landing
bucket_staging_path = /staging

##############
# Query Exec #
##############
# cache settings
cache_dataframes = false
clean_past_cache_delay = 60 seconds
cache_expiry = 30 seconds
spark_sql_print_explain = false

############
# Pipeline #
############

# pipeline settings
trigger_time = 10 seconds
# this is the maxOpIndex number for an interval
compaction_record_interval = 10

# kafka settings
kafka_bootstrap_servers = "localhost:9092"
kafka_topic = "backbeat"

#############
# Compactor #
#############

# compaction
# for cache tests, make this longer than the cache expiry so after running the compaction, the cache
# will also be out of date
landing_purge_tolerance = 35 seconds


# graphite params
graphite {
  hostname = "" // turns search metrics off
  port = 2003
}
